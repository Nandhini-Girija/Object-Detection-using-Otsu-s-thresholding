# -*- coding: utf-8 -*-
"""Digital_Image_Processing_Mini_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GyAQTOqY0MBmlttKudEvgb9rOKudFVL
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

def laplacian_filter(image):
    # Convert image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Define Laplacian filter kernel
    kernel = np.array([[-1, -1, -1],
                       [-1, 8, -1],
                       [-1, -1, -1]])

    # Pad the image to handle border pixels
    padded_image = np.pad(gray, ((1, 1), (1, 1)), mode='constant')

    # Initialize output image
    output = np.zeros_like(gray, dtype=np.float32)

    # Convolve the image with the Laplacian kernel
    for i in range(gray.shape[0]):
        for j in range(gray.shape[1]):
            output[i, j] = np.sum(padded_image[i:i+3, j:j+3] * kernel)


    return output

def high_boost_filter(image, A=1.15):


    # Convert image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Normalize the image
    normalized_image=gray.astype(np.float32)/255


    # Apply Laplacian filter (high-pass filter)
    high_pass_filtered = laplacian_filter(image)
    print(high_pass_filtered.shape)

    # Normalize the High pass filtered Image
    normalized_high_pass_filtered_image=high_pass_filtered.astype(np.float32)/255


    # Scale the original image
    original_scaled = normalized_image.astype(np.float32) * A

    # Add the scaled original image to the high-pass filtered image
    normalized_high_pass_scaled = high_pass_filtered + original_scaled

    # Result


    result=(normalized_high_pass_scaled*255).astype(np.uint8)

    # Show the  original  image
    plt.figure(figsize=(10, 5))
    plt.imshow(image, cmap='gray')
    plt.title('Original Image')
    plt.axis('off')


    plt.show()

    # Show the  High Pass filtered image
    plt.figure(figsize=(10, 5))
    plt.imshow(high_pass_filtered, cmap='gray')
    plt.title('High Pass filtered Image')
    plt.axis('off')

    plt.show()

    return result

# Read input image
image = cv2.imread('text.tif')
high_pass_filtered = laplacian_filter(image)

# Apply high boost filter with A=1.5
result = high_boost_filter(image)

# Display original and filtered images using matplotlib
plt.figure(figsize=(10, 5))

plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.title('Original')
plt.axis('off')
plt.show()
plt.figure(figsize=(10, 5))
plt.imshow(result, cmap='gray')
plt.title('High Boost Filtered (A=1.15)')
plt.axis('off')

plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load an image
original_image = cv2.imread('text.tif')

# Convert image to grayscale
gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)

# Apply Gaussian blur with kernel size 7x7
blurred_image = cv2.GaussianBlur(gray_image, (7, 7), 0)

# Subtract blurred image from the original image without using built-in function
subtracted_image = np.zeros_like(gray_image)
for i in range(gray_image.shape[0]):
    for j in range(gray_image.shape[1]):
        subtracted_image[i, j] = gray_image[i, j] - blurred_image[i, j]

# Define the constant k
k = 1.5

# Multiply the subtracted image by the constant k without using built-in function
multiplied_image = np.zeros_like(subtracted_image)
for i in range(subtracted_image.shape[0]):
    for j in range(subtracted_image.shape[1]):
        multiplied_image[i, j] = subtracted_image[i, j] * k

# Add the multiplied image to the original image without using built-in function
result_image = np.zeros_like(gray_image)
for i in range(gray_image.shape[0]):
    for j in range(gray_image.shape[1]):
        result_image[i, j] = gray_image[i, j] + multiplied_image[i, j]

# Display the original, subtracted, multiplied, and final images using Matplotlib
plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
plt.title('Original Image')
plt.axis('off')

plt.subplot(2, 2, 2)
plt.imshow(subtracted_image, cmap='gray')
plt.title('Subtracted Image')
plt.axis('off')

plt.subplot(2, 2, 3)
plt.imshow(multiplied_image, cmap='gray')
plt.title('Multiplied Image (k = {})'.format(k))
plt.axis('off')

plt.subplot(2, 2, 4)
plt.imshow(result_image, cmap='gray')
plt.title('Result Image')
plt.axis('off')

plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

def otsu_threshold(image, threshold):
    # Number of pixels in the image
    total_pixels = image.size

    # Class probabilities
    class1_prob = np.sum(hist[:threshold+1])
    class2_prob = np.sum(hist[threshold+1:])

    # Class means
    intensity_levels = np.arange(0, 256)
    class1_mean = np.sum(intensity_levels[:threshold+1] * hist[:threshold+1]) / class1_prob
    class2_mean = np.sum(intensity_levels[threshold+1:] * hist[threshold+1:]) / class2_prob

    # Class variances
    class1_variance = np.sum(((intensity_levels[:threshold+1] - class1_mean) ** 2) * hist[:threshold+1]) / class1_prob
    class2_variance = np.sum(((intensity_levels[threshold+1:] - class2_mean) ** 2) * hist[threshold+1:]) / class2_prob

    # Between-class variance
    between_class_var = class1_prob * class2_prob * (class1_mean - class2_mean) ** 2

    # Within-class variance
    within_class_var = class1_prob * class1_variance + class2_prob * class2_variance

    return class1_prob, class2_prob, class1_mean, class2_mean, class1_variance, class2_variance, between_class_var,within_class_var

def threshold_image(image, threshold):
    # Convert image to grayscale if it's not already
    if len(image.shape) == 3:
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray_image = image

    # Create a binary mask based on the threshold
    binary_mask = np.zeros_like(gray_image)
    binary_mask[gray_image > threshold] = 255

    return binary_mask

def compute_normalized_histogram(image):
    # Convert the image to grayscale if it's in color
    if len(image.shape) > 2:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Compute the histogram
    hist = cv2.calcHist([image], [0], None, [256], [0,256])

    # Normalize the histogram
    hist_normalized = hist / np.sum(hist)

    return hist_normalized
# Read the image
image = cv2.imread('cg3.jpg', cv2.IMREAD_GRAYSCALE)

# Histogram calculation
hist = compute_normalized_histogram(image)


plt.plot(hist, color='gray')
plt.xlabel('Pixel Intensity')
plt.ylabel('Frequency')
plt.title('Histogram of the Image')

plt.show()

# Calculate global mean
global_mean = np.mean(image)

# Calculate global variance
global_variance = np.var(image)

# Plot the histogram

print("Global Mean:", global_mean)
print("Global Variance:", global_variance)
between_class_var=[]
within_class_var=[]
for k in range(1,255):
  _,_,_,_,_,_,b,w=(otsu_threshold(image,k))
  #print(b,w)
  between_class_var.append(b)
  within_class_var.append(w)

min_within_class_variance = min(within_class_var)
min_index_within_class = within_class_var.index(min_within_class_variance)

print("Minimum Within-class Variance:", min_within_class_variance)
print("Index of Minimum Value for Within-class Variance:", min_index_within_class)

max_between_class_variance = max(between_class_var)
max_index_between_class = between_class_var.index(max_between_class_variance)
print("Maximum Between-class Variance:", max_between_class_variance)
print("Index of Maximum Value for Between-class Variance:", max_index_between_class)
threshold_value=max_index_between_class
segmented_image = threshold_image(image, threshold_value)
plt.figure()
plt.imshow(image,cmap='gray')
plt.title('Original Image')
plt.show()
plt.figure()
plt.imshow(segmented_image,cmap='gray')
plt.title('Segmented Image')
plt.show()


def evaluate_segmentation(segmented_image, ground_truth_image):

    # Assuming segmented_image and ground_truth_image are both binary images
    TP = np.sum(np.logical_and(segmented_image == 255, ground_truth_image == 255))
    TN = np.sum(np.logical_and(segmented_image == 0, ground_truth_image == 0))
    FP = np.sum(np.logical_and(segmented_image == 255, ground_truth_image == 0))
    FN = np.sum(np.logical_and(segmented_image == 0, ground_truth_image == 255))
    return TP, TN, FP, FN

# Assuming ground_truth_image is already loaded
ground_truth_image = cv2.imread('cg4.jpg', cv2.IMREAD_GRAYSCALE)
plt.figure()
plt.imshow(ground_truth_image,cmap='gray')
plt.title('Ground Truth Image')
plt.show()
# Evaluate segmentation
TP, TN, FP, FN = evaluate_segmentation(segmented_image, ground_truth_image)

print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

Accuracy=(TP+TN)/(TP+TN+FP+FN)
Sensitivity=TP/(TP+FN)
Specificity=TN/(TN+FP)
print('Accuracy of the Segmentation',Accuracy)
print('Sensitivity of the Segmentation',Sensitivity)
print('Specificity of the Segmentation',Specificity)


from sklearn.metrics import confusion_matrix
import seaborn as sns

# Define the labels for the confusion matrix
labels = [ 'Positive', 'Negative ']

# Define the confusion matrix data
cm = np.array([[TN, FP], [FN, TP]])

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
print(image.shape)



import cv2
import numpy as np
import matplotlib.pyplot as plt


# Define the  kernel
kernel = np.ones((5, 5), np.uint8)
kerrnel2 = np.ones((3, 3), np.uint8)

def dilate(image, kernel):
    rows, cols = image.shape
    output_image = np.zeros((rows, cols), dtype=np.uint8)
    k_rows, k_cols = kernel.shape

    # Iterate over each pixel in the image
    for i in range(rows):
        for j in range(cols):
            # If the current pixel is white, set all pixels in the kernel's neighborhood to white in the output image
            if image[i, j] == 255:
                for m in range(k_rows):
                    for n in range(k_cols):
                        if i - k_rows//2 + m >= 0 and i - k_rows//2 + m < rows and j - k_cols//2 + n >= 0 and j - k_cols//2 + n < cols:
                            output_image[i - k_rows//2 + m, j - k_cols//2 + n] = 255

    return output_image

def erode(image, kernel):
    rows, cols = image.shape
    output_image = np.zeros((rows, cols), dtype=np.uint8)
    k_rows, k_cols = kernel.shape

    # Iterate over each pixel in the image
    for i in range(rows):
        for j in range(cols):
            # Check if all pixels in the kernel's neighborhood are white
            all_white = True
            for m in range(k_rows):
                for n in range(k_cols):
                    if i - k_rows//2 + m >= 0 and i - k_rows//2 + m < rows and j - k_cols//2 + n >= 0 and j - k_cols//2 + n < cols:
                        if image[i - k_rows//2 + m, j - k_cols//2 + n] != 255:
                            all_white = False
                            break
                if not all_white:
                    break
            # If all pixels in the kernel's neighborhood are white, set the current pixel to white in the output image
            if all_white:
                output_image[i, j] = 255

    return output_image


#Opening
dilated_image = dilate(segmented_image, kernel)
eroded_image = erode(dilated_image, kernel)


# Plot the original and dilated images
plt.figure(figsize=(10, 10))
plt.subplot(1, 3,1 )
plt.imshow(segmented_image, cmap='gray')
plt.title('Segmented Image')
plt.subplot(1, 3, 2)
plt.imshow(dilated_image, cmap='gray')
plt.title('Dilated Image ')

plt.subplot(1, 3, 3)
plt.imshow(eroded_image, cmap='gray')
plt.title('Eroded Image')
plt.show()
#Closing
eroded_image2=erode(eroded_image,kernel)
dialeted_image2=dilate(eroded_image2,kernel)

plt.figure(figsize=(10, 5))

plt.subplot(1, 3, 1)
plt.imshow(eroded_image, cmap='gray')
plt.title('Opened Image')


plt.subplot(1, 3, 2)
plt.imshow(eroded_image2, cmap='gray')
plt.title('Eroded Image 2')


plt.subplot(1, 3, 3)
plt.imshow(dialeted_image2, cmap='gray')
plt.title(' Dialation Image 2')
plt.show()

#Opening
dialeted_image3=dilate(dialeted_image2,kernel)
eroded_image3=erode(dialeted_image3,kernel)




plt.figure(figsize=(10, 5))
plt.subplot(1, 3, 1)
plt.imshow(dialeted_image2, cmap='gray')
plt.title('Closed Image')
plt.subplot(1, 3,2)
plt.imshow(dialeted_image3, cmap='gray')
plt.title(' Dialation Image 3')
plt.subplot(1, 3, 3)
plt.imshow(eroded_image3, cmap='gray')
plt.title('Eroded Image 3')
plt.show()

# Evaluate segmentation
TP, TN, FP, FN = evaluate_segmentation(eroded_image3, ground_truth_image)

print("True Positive (TP):", TP)
print("True Negative (TN):", TN)
print("False Positive (FP):", FP)
print("False Negative (FN):", FN)

Accuracy=(TP+TN)/(TP+TN+FP+FN)
Sensitivity=TP/(TP+FN)
Specificity=TN/(TN+FP)
print('Accuracy of the Segmentation',Accuracy)
print('Sensitivity of the Segmentation',Sensitivity)
print('Specificity of the Segmentation',Specificity)


from sklearn.metrics import confusion_matrix
import seaborn as sns

labels = [ 'Positive', 'Negative ']

cm = np.array([[TN, FP], [FN, TP]])

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
print(image.shape)